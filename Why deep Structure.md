---
typora-copy-images-to: img
---

### Why Deep Structure?

#### 1ã€Can shallow network fit any function? 

 **Given a shallow network structure with one hidden layer with ReLU activation and linear output**

![1-1](https://github.com/haoyuheng/MLDS_notebook/blob/master/img/1-1.png)

**Given a L-Lipschitz function **$f^*$

Lipschitzï¼ˆåˆ©æ™®å¸ŒèŒ¨ï¼‰è¿ç»­å®šä¹‰ï¼š 
æœ‰å‡½æ•°$f(x)$ï¼Œå¦‚æœå­˜åœ¨ä¸€ä¸ªå¸¸é‡Lï¼Œä½¿å¾—å¯¹$f(x)$å®šä¹‰åŸŸä¸Šï¼ˆå¯ä¸ºå®æ•°ä¹Ÿå¯ä»¥ä¸ºå¤æ•°ï¼‰çš„ä»»æ„ä¸¤ä¸ªå€¼æ»¡è¶³å¦‚ä¸‹æ¡ä»¶ï¼š 
<a href="https://www.codecogs.com/eqnedit.php?latex=|f(x_1)-f(x_2)|\leq&space;L*|x_1-x_2|" target="_blank"><img src="https://latex.codecogs.com/gif.latex?|f(x_1)-f(x_2)|\leq&space;L*|x_1-x_2|" title="|f(x_1)-f(x_2)|\leq L*|x_1-x_2|" /></a>
é‚£ä¹ˆç§°å‡½æ•°$f(x)$æ»¡è¶³Lipschitzè¿ç»­æ¡ä»¶ï¼Œå¹¶ç§°Lä¸º$f(x)$çš„Lipschitzå¸¸æ•°ã€‚ 

Lipschitzè¿ç»­æ¯”ä¸€è‡´è¿ç»­è¦å¼ºã€‚å®ƒé™åˆ¶äº†å‡½æ•°çš„å±€éƒ¨å˜åŠ¨å¹…åº¦ä¸èƒ½è¶…è¿‡æŸå¸¸é‡ã€‚

**How many neurons are needed to approximate$f^*$?** 

Given a small number $\varepsilon > 0$
$$
\exists  f\epsilon N(K) \max_{0\leq x \leq 1}|f(x)-f^*(x)|\leq \varepsilon
$$
$f\epsilon N(K)$ï¼šThe function space defined by the network with K neurons.

The difference between$f(x)$ and $ğ‘“
^âˆ—( ğ‘¥)$ is smaller than $\varepsilon$

![1-2](https://github.com/haoyuheng/MLDS_notebook/blob/master/img/1-2.png)

**So $2L/\varepsilon$ Relu neurons shallow network can fit any  L-Lipschitz function.**

#### 2ã€Potential of Deep



#### 3ã€Is Deep better than Shallow?
