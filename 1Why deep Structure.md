---
typora-copy-images-to: img
---

### 1	Why Deep Structure?

#### 1.1		Can shallow network fit any function? 

 **Given a shallow network structure with one hidden layer with ReLU activation and linear output**

![1-1](https://github.com/haoyuheng/MLDS_notebook/blob/master/img/1-1.png?raw=true)

**Given a L-Lipschitz function $f^*$**

Lipschitzï¼ˆåˆ©æ™®å¸ŒèŒ¨ï¼‰è¿ç»­å®šä¹‰ï¼š 
æœ‰å‡½æ•°$f(x)$ï¼Œå¦‚æœå­˜åœ¨ä¸€ä¸ªå¸¸é‡Lï¼Œä½¿å¾—å¯¹$f(x)$å®šä¹‰åŸŸä¸Šï¼ˆå¯ä¸ºå®æ•°ä¹Ÿå¯ä»¥ä¸ºå¤æ•°ï¼‰çš„ä»»æ„ä¸¤ä¸ªå€¼æ»¡è¶³å¦‚ä¸‹æ¡ä»¶ï¼š 
$$
|fï¼ˆx_1ï¼‰-f(x_2)|\leq L*|x_1-x_2|
$$
é‚£ä¹ˆç§°å‡½æ•°$f(x)$æ»¡è¶³Lipschitzè¿ç»­æ¡ä»¶ï¼Œå¹¶ç§°Lä¸º$f(x)$çš„Lipschitzå¸¸æ•°ã€‚ 

Lipschitzè¿ç»­æ¯”ä¸€è‡´è¿ç»­è¦å¼ºã€‚å®ƒé™åˆ¶äº†å‡½æ•°çš„å±€éƒ¨å˜åŠ¨å¹…åº¦ä¸èƒ½è¶…è¿‡æŸå¸¸é‡ã€‚

**How many neurons are needed to approximate$f^*$?** 

Given a small number $\varepsilon > 0$
$$
\exists  f\epsilon N(K) \max_{0\leq x \leq 1}|f(x)-f^*(x)|\leq \varepsilon
$$
$f\epsilon N(K)$ï¼šThe function space defined by the network with K neurons.

The difference between$f(x)â€‹$ and $ğ‘“
^âˆ—( ğ‘¥)â€‹$ is smaller than $\varepsilonâ€‹$

![1-2](https://github.com/haoyuheng/MLDS_notebook/blob/master/img/1-2.png?raw=true)

**So $2L/\varepsilon$ Relu neurons shallow network can fit any  L-Lipschitz function.**

#### 1.2		Potential of Deep

![1-5](https://github.com/haoyuheng/MLDS_notebook/blob/master/img/1-5.png?raw=true)

![1-4](https://github.com/haoyuheng/MLDS_notebook/blob/master/img/1-4.png?raw=true)

**If K is width, H is depth . We can have at least $K^H $pieces**

Depth has much larger influence than width.

ä¹Ÿå°±æ˜¯è¯´å½“NNçš„layerçš„å±‚æ•°ä¸ºHï¼Œæ¯ä¸€å±‚æœ‰Kä¸ªNeuralçš„æ—¶å€™ï¼ŒNNå¯ä»¥è¡¨ç¤ºä¸€ä¸ªæœ‰$K^H $piecesçš„åˆ†æ®µå‡½æ•°ã€‚

æ‰€ä»¥Depthçš„å¯¹NNè¡¨ç¤ºèƒ½åŠ›çš„å½±å“è¦å¤§äºWidthã€‚

#### 1.3		Is Deep better than Shallow?

![1-3](https://github.com/haoyuheng/MLDS_notebook/blob/master/img/1-3.png?raw=true)