## Generalizationï¼ˆæ³›åŒ–ï¼‰

### Generalization Ability

**Generalization Gap**:  difference between training error and testing error

**Capacity of model**: "size" of the your model, ä¹Ÿå¯ä»¥ç†è§£ä¸ºmodelçš„parameterçš„è§„æ¨¡ï¼Œå¯ä»¥ä½¿ç”¨VCç»´æ¥åº¦é‡

ä¸€èˆ¬æ¥è¯´ï¼Œcapacity è¶Šå¤§ï¼Œä¼šå­˜åœ¨overfitçš„ç°è±¡ï¼Œå¯¼è‡´Generalization Gapå¢å¤§ï¼›ä½†æ˜¯å¯¹deep learningæ¥è¯´ï¼Œä¸€èˆ¬æ¥è¯´å¹¶ä¸å®¹æ˜“overfitï¼ˆæœ‰æ—¶ä¹Ÿä¼šï¼‰ã€‚å¦‚ï¼š

1ã€åœ†åœˆçš„é¢ç§¯å•è¡¨paramsçš„è§„æ¨¡ï¼Œé¢ç§¯è¶Šå¤§ï¼Œparamsè¶Šå¤šï¼Œå¯ä»¥çœ‹å‡ºå¾€å¾€æ˜¯paramsè¶Šå¤šçš„networkå¾€å¾€è¡¨ç°çš„æ›´å¥½ã€‚[AN ANALYSIS OF DEEP NEURAL NETWORK MODELS FOR PRACTICAL APPLICATIONS](https://arxiv.org/pdf/1605.07678.pdf)

![3-1](/home/MLDS_notebook/assets/3-1.png)

2ã€å¯¹fully connected networkæ¥è¯´ï¼Œlayerè¶Šå¤šï¼Œparamsè¶Šå¤šï¼Œä½†ç”±ä¸‹å›¾å¯çŸ¥å±‚æ•°å¢åŠ æ—¶å¹¶æœªå­˜åœ¨overfitçš„æƒ…å†µã€‚[Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes](https://arxiv.org/pdf/1706.10239.pdf)

![3-2](/home/MLDS_notebook/assets/3-2.png)

3ã€Google Brainçš„æ–‡ç« ï¼Œä»ä¸‹å›¾å¯ä»¥çœ‹å‡ºï¼ŒNumber of weightså¢å¤§çš„æ—¶å€™gapçš„æœ€å°å€¼å¹¶ä¸ä¼šé™ä½ã€‚[SENSITIVITY AND GENERALIZATION IN NEURAL NETWORKS: AN EMPIRICAL STUDY](https://arxiv.org/pdf/1802.08760.pdf)

![3-3](/home/MLDS_notebook/assets/3-3.png)

**Concluding Remarks**

â€¢ The capacity of deep model is large.
â€¢ However, it does not overfit!
â€¢ The reason is not clear yet.

### Indicator of Generalization

#### Sensitivity

**Definition**: Given a network ğ‘“, the sensitivity of a data point x is the Frobenius norm of the Jacobian.

![3-4](/home/MLDS_notebook/assets/3-4.png)

Sensitivity of x =![3-5](/home/MLDS_notebook/assets/3-5.png)

#### Sharpness

**Concluding Remarks**

â€¢ Good generalization are associated with sensitivity
â€¢ Good generalization are associated with flatness (?)
â€¢ Understanding the indicator for generalization helps us develop algorithm in the future