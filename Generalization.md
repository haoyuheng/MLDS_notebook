## 3	Generalizationï¼ˆæ³›åŒ–ï¼‰

### 3.1		Generalization Ability

**Generalization Gap**:  difference between training error and testing error

**Capacity of model**: "size" of the your model, ä¹Ÿå¯ä»¥ç†è§£ä¸ºmodelçš„parameterçš„è§„æ¨¡ï¼Œå¯ä»¥ä½¿ç”¨VCç»´æ¥åº¦é‡

ä¸€èˆ¬æ¥è¯´ï¼Œcapacity è¶Šå¤§ï¼Œä¼šå­˜åœ¨overfitçš„ç°è±¡ï¼Œå¯¼è‡´Generalization Gapå¢å¤§ï¼›ä½†æ˜¯å¯¹deep learningæ¥è¯´ï¼Œä¸€èˆ¬æ¥è¯´å¹¶ä¸å®¹æ˜“overfitï¼ˆæœ‰æ—¶ä¹Ÿä¼šï¼‰ã€‚å¦‚ï¼š

1ã€åœ†åœˆçš„é¢ç§¯å•è¡¨paramsçš„è§„æ¨¡ï¼Œé¢ç§¯è¶Šå¤§ï¼Œparamsè¶Šå¤šï¼Œå¯ä»¥çœ‹å‡ºå¾€å¾€æ˜¯paramsè¶Šå¤šçš„networkå¾€å¾€è¡¨ç°çš„æ›´å¥½ã€‚[AN ANALYSIS OF DEEP NEURAL NETWORK MODELS FOR PRACTICAL APPLICATIONS](https://arxiv.org/pdf/1605.07678.pdf)

![3-1](https://github.com/haoyuheng/MLDS_notebook/blob/master/assets/3-1.png?raw=true)

2ã€å¯¹fully connected networkæ¥è¯´ï¼Œlayerè¶Šå¤šï¼Œparamsè¶Šå¤šï¼Œä½†ç”±ä¸‹å›¾å¯çŸ¥å±‚æ•°å¢åŠ æ—¶å¹¶æœªå­˜åœ¨overfitçš„æƒ…å†µã€‚[Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes](https://arxiv.org/pdf/1706.10239.pdf)

![3-2](https://github.com/haoyuheng/MLDS_notebook/blob/master/assets/3-2.png?raw=true)

3ã€Google Brainçš„æ–‡ç« ï¼Œä»ä¸‹å›¾å¯ä»¥çœ‹å‡ºï¼ŒNumber of weightså¢å¤§çš„æ—¶å€™gapçš„æœ€å°å€¼å¹¶ä¸ä¼šé™ä½ã€‚[SENSITIVITY AND GENERALIZATION IN NEURAL NETWORKS: AN EMPIRICAL STUDY](https://arxiv.org/pdf/1802.08760.pdf)

![3-3](https://github.com/haoyuheng/MLDS_notebook/blob/master/assets/3-3.png?raw=true)

**Concluding Remarks**

â€¢ The capacity of deep model is large.

â€¢ However, it does not overfit!

â€¢ The reason is not clear yet.

### 3.2		Indicator of Generalization

#### Sensitivity

**Definition**: Given a network ğ‘“, the sensitivity of a data point x is the Frobenius norm of the Jacobian.

![3-4](https://github.com/haoyuheng/MLDS_notebook/blob/master/assets/3-4.png?raw=true)

Sensitivity of x isï¼š![3-5](https://github.com/haoyuheng/MLDS_notebook/blob/master/assets/3-5.png?raw=true)

It is not surprise that sensitivity is related to generalization.

Sensitivity ç›´è§‚ä¸Šæ˜¯xå˜åŒ–å¯¹yçš„å˜åŒ–çš„å½±å“ï¼ŒSensitivityè¶Šå¤§ï¼Œğ‘“åœ¨xç‚¹é™¡å³­ï¼Œyçš„å˜åŒ–è¶Šå¤§ï¼›åä¹‹ï¼ŒSensitivityè¶Šå°ï¼Œğ‘“åœ¨xç‚¹å¹³æ»‘ï¼Œyçš„å˜åŒ–è¶Šå°ã€‚

**Regularization** is kind of minimizing sensitivity.

Regularizationå¯ä»¥å¹³æ»‘ğ‘“ã€‚

å¯¹ä¸€ä¸ªè®­ç»ƒå¥½çš„modelï¼Œåœ¨training dataé™„è¿‘çš„Sensitivityä¼šæ¯”è¾ƒå°ï¼Œåä¹‹åœ¨training dataæ²¡æœ‰å‡ºç°çš„åœ°æ–¹Sensitivityè¾ƒå°ã€‚ï¼ˆè¿˜æ˜¯å‚ç…§Google Brainå…³äºSensitivityçš„æ–‡ç« ï¼‰

1ã€å¦‚ä¸‹å›¾ï¼Œé»‘è‰²ä»£è¡¨ä¸€ä¸ªéšæœºçš„æ¤­åœ†å‡½æ•°ï¼Œå¹¶ä¸ç»è¿‡ä»»ä½•training dataï¼ŒSensitivityåœ¨æ¯ä¸ªç‚¹éƒ½è¾ƒé«˜ã€‚è€Œçº¢è‰²æ¤­åœ†åœ¨ç»è¿‡äº†training dataçš„ä¸‰ä¸ªç‚¹ï¼ŒSensitivityè¾ƒä½ã€‚

![3-6](https://github.com/haoyuheng/MLDS_notebook/blob/master/assets/3-6.png?raw=true)

2ã€æ¯”è¾ƒäº†trainingå‰åï¼Œmodelçš„Sensitivityåˆ†å¸ƒï¼Œå¯è§åœ¨æœ‰training dataçš„ä½ç½®ï¼Œè®­ç»ƒåmodelçš„Sensitivityè¾ƒå°ï¼Œmodelæ¯”è¾ƒå¹³æ»‘ã€‚å›¾ä¸­è‰²å—çš„å¤§å°å¯ä»¥ç†è§£ä¸ºRelu ç½‘ç»œpiecewiseçš„é•¿åº¦ï¼Œè‰²å—è¶Šå¤§ï¼Œè¶Šå¹³æ»‘ã€‚

![3-7](https://github.com/haoyuheng/MLDS_notebook/blob/master/assets/3-7.png?raw=true)

**A strong relationship between the Jacobian norm and generalization**.

ä»ä¸‹å›¾å¯ä»¥çœ‹åˆ°ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œ Jacobian norm è¶Šå°ï¼ŒGeneralization gapè¶Šå°ã€‚

![3-8](https://github.com/haoyuheng/MLDS_notebook/blob/master/assets/3-8.png?raw=true)

#### Sharpness

æˆ‘ä»¬æŠŠminimaåˆ†ä¸ºFlat Minimaå’ŒSharp Minimaï¼Œä»ä¸‹å›¾å¯ä»¥ç›´è§‚çš„çœ‹å‡ºï¼ŒFlat Minimaå¤„å…·æœ‰å¥½çš„Generalizationï¼ˆTraining Losså’ŒTesting Lossçš„Gapè¾ƒå°ï¼‰ï¼Œè€ŒSharp Minimaå¯¹åº”ä¸bad Generalization.

![3-9](https://github.com/haoyuheng/MLDS_notebook/blob/master/assets/3-9.png?raw=true)

é‚£ä¹ˆå¦‚ä½•å®šä¹‰ä¸€ä¸ªç‚¹çš„Sharpnesså‘¢ï¼Ÿ

**Definition of Sharpness**

![3-10](https://github.com/haoyuheng/MLDS_notebook/blob/master/assets/3-10.png?raw=true)

**Batch Size**

ä¸€èˆ¬æ¥è¯´ï¼ŒBatch Size è¶Šå°ï¼ŒGeneralizationæ•ˆæœè¶Šå¥½ã€‚

![3-11](https://github.com/haoyuheng/MLDS_notebook/blob/master/assets/3-11.png?raw=true)

**Batch Size v.s. Sharpness**

å¯ä»¥çœ‹å‡ºï¼Œéšç€batch sizeå¢å¤§ï¼ŒSharpnessä¹Ÿä¼šå¢å¤§ï¼Œè€ŒGeneralizationçš„æ•ˆæœå˜å·®ã€‚

![3-12](https://github.com/haoyuheng/MLDS_notebook/blob/master/assets/3-12.png?raw=true)

**Concluding Remarks**

â€¢ Good generalization are associated with sensitivity

â€¢ Good generalization are associated with flatness (?)

â€¢ Understanding the indicator for generalization helps us develop algorithm in the future